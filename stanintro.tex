\documentclass[xcolor=dvipsnames]{beamer}
\makeatletter\def\Hy@xspace@end{}\makeatother
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol, amsthm}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage[authoryear]{natbib}
%\newcommand{\newblock}{}  %needed to make beamer and natbib play nice
\usepackage{tikz}
%\usetheme{Boadilla}
%\usecolortheme{lily}
 \usecolortheme[named=Red]{structure}
% \setbeamercolor{structure}{bg=black}
% \setbeamercolor{structure}{fg=Goldenrod}
% \setbeamercolor{title}{bg=Black}
% \setbeamercolor{frametitle}{bg=Black, fg=Goldenrod}
% \setbeamercolor{title in head/foot}{fg=Black, bg=Goldenrod}
% \setbeamercolor{author in head/foot}{fg=Goldenrod, bg=Black}
% \setbeamercolor{institute in head/foot}{fg=Goldenrod, bg=Black}
% \setbeamercolor{date in head/foot}{fg=Goldenrod, bg=Black}
\setbeamercovered{transparent=0}
\beamertemplatenavigationsymbolsempty

\title[Stan for Spatial]{Introduction to Stan for \\
Markov Chain Monte Carlo}

%\subtitle{}
\author[Matt Simpson]{Matthew Simpson}
\institute[Mizzou Statistics]{Department of Statistics, University of Missouri}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

% \begin{frame}[fragile]
% \frametitle{Abstract}
% \centering
% In the first part of this talk I introduce Hamiltonian Monte Carlo (HMC) and its implementation in Stan. The focus is on building intuition and not on theory, e.g. for how HMC works, how Stan solves many of the problems typically associated with implementing HMC, and for why HMC and Stan are attractive alternatives to Gibbs based sampling algorithms. In the second part of the talk, I provide a brief introduction to using Stan with the \verb0rstan0 \verb0R0 package. I provide several examples in this portion of the talk, guidance for interpreting Stan's warning and error messages, and intuition for how to construct better HMC samplers using Stan.
% \end{frame}

\begin{frame}
\frametitle{Stan is...}

\begin{columns}[c]
 \begin{column}[c]{0.5\textwidth}

\begin{itemize}
\item[]
\begin{center}
\includegraphics[width=0.5\textwidth]{stan.jpg}
\end{center}
\end{itemize}
\end{column}

\begin{column}[c]{0.5\textwidth}
\begin{itemize}
\item[] \textbf{Stanislaw Ulam}, inventor of Monte Carlo methods.\\
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}

\begin{itemize}
\item[] \url{http://mc-stan.org/}

\vspace{0.2cm}

\item[] A \textbf{probabilistic programming language} that implements {\color{red}\textbf{Hamiltonian Monte Carlo (HMC)}}, variational Bayes, and (penalized) maximum likelihood estimation.\\

\vspace{0.2cm}

\item[] Available on Linux, Mac, and Windows with interfaces in \textbf{R}, \textbf{Python}, shell (command line), MATLAB, Julia, Stata, and Mathematica.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Markov chain Monte Carlo (MCMC)}
Goal: sample from some target density ${\color{red}\pi(\bm{q})}$.\\

\vspace{0.5cm}

Create a Markov chain with transition density $k(\bm{q}'|\bm{q})$.
\begin{itemize}
\item Start with arbitrary $\bm{q}^{(0)}$ and repeatedly sample $\bm{q}^{(t+1)} \sim k(\bm{q}'|\bm{q}^{(t)})$.\\

\vspace{0.2cm}

\item Under some conditions $\bm{q}^{(t)} \to \bm{q}$ in distribution.

\vspace{0.2cm}

\item Additionally with {\color{red}geometric ergodicity}:
\begin{align*}
  \frac{1}{T}\sum_{t=1}^T f(\bm{q}^{(t)}) \to \mathrm{N}\left(\mathrm{E}[f(\bm{q})], \frac{\mathrm{var}[f(\bm{q})]}{ESS}\right).
\end{align*}
\end{itemize}

\vspace{0.5cm}

Auxillary variable MCMC: construct a variable $\bm{p}$ with joint density $\pi(\bm{q},\bm{p}) = \pi(\bm{p}|\bm{q}){\color{red}\pi(\bm{q})}$.
\begin{itemize}
\item Construct a Markov chain for $(\bm{q}, \bm{p})$ and throw away the sampled $\bm{p}^{(t)}$s.
\vspace{0.2cm}
\item Ex: data augmentation, slice sampling, HMC, etc.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{HMC in Theory}
Construct $\bm{p}$ in a special way. Let $\bm{q},\bm{p}\in \Re^n$ and:
\begin{itemize}
\item[] $V(\bm{q})\phantom{,\bm{p}} = -\log {\color{red}\pi(\bm{q})}$ --- \emph{potential energy}.
\item[] $T(\bm{q}, \bm{p}) = - \log \pi(\bm{p} | \bm{q})$ --- \emph{kinetic energy}.
\item[] $H(\bm{q}, \bm{p}) = V(\bm{q}) + T(\bm{q}, \bm{p})$ --- \emph{Hamiltonian}, total energy.
\end{itemize}
where $\bm{q}$ denotes {\color{red}\emph{position}} and $\bm{p}$ denotes {\color{red}\emph{momentum}}.

\vspace{0.3cm}

Energy-preserving evolution in time is defined by Hamilton's equations:
\begin{align*}
\frac{\mathrm{d}\bm{p}}{\mathrm{d} t} = - \frac{\partial H}{\partial \bm{q}}; && \frac{\mathrm{d}\bm{q}}{\mathrm{d} t} = + \frac{\partial H}{\partial \bm{p}}.
\end{align*}
How to implement HMC (in theory):
\begin{enumerate}
\item Sample momenta variables: $\bm{p}' \sim \pi(\bm{p}|\bm{q}^{(t)})$.
\item Run Hamiltonian evolution forward in time from $(\bm{q}^{(t)}, \bm{p}')$ for a some amount of {\color{red}\emph{integration time}} to obtain $(\bm{q}^{(t+1)}, \bm{p}^{(t+1)})$.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{HMC in Pictures}
\begin{center}
\includegraphics[height=0.5\textheight]{hmc.png}
\end{center}

HMC samples a level set, then moves along that set.\\~\\


Long integration time $\implies$ $\approx$ zero autocorrelation in the chain.\\~\\

(Picture stolen from \url{https://arxiv.org/pdf/1601.00225.pdf})
\end{frame}

\begin{frame}{fragile}
\frametitle{HMC in Practice}
To implement HMC for a \textbf{differentiable} target ${\color{red}\pi(\bm{q})}$ you need:
\vspace{0.2cm}
\begin{enumerate}
\item No discrete valued parameters in $\bm{q}$.
\begin{itemize}
\item Usually can integrate them out, e.g. mixture models.
\end{itemize}
\vspace{0.2cm}
\item No constrained parameters in $\bm{q}$.
\begin{itemize}
\item Stan: transform and compute the log-Jacobian automatically.
\end{itemize}
\vspace{0.2cm}
\item The gradient vector of $\log {\color{red}\pi(\bm{q})}$.
\begin{itemize}
\item Stan: use C++ autodiff library to do this automatically and accurately.
\end{itemize}
\vspace{0.2cm}
\item Choose a kinetic energy, i.e. $\pi(\bm{p}|\bm{q})$.
\begin{itemize}
\item Stan: $\mathrm{N}(\bm{0}, \bm{M})$ and tune $\bm{M}$ during warmup. (Typical HMC)
\item More intelligent: $\bm{M}(\bm{q})$. (Riemannian HMC; future Stan)
\end{itemize}
\end{enumerate}
\begin{align*}
{\color{red}\Huge{\vdots}}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{HMC in Practice (continued)}
To implement HMC for a \textbf{differentiable} target ${\color{red}\pi(\bm{q})}$ you need:
\vspace{0.2cm}
\begin{enumerate}
\item[5.] Numerical integrator for Hamilton's equations.
\begin{itemize}
\item Need to make an adjustment to the Hamiltonian flow and use a Metropolis correction to ensure detailed balance.
\vspace{0.2cm}
\item Typically use leapfrog integration $\implies$ how many leapfrog steps?
\vspace{0.2cm}
\item Stan: adapt number of steps to hit a target Metropolis acceptance rate.
\end{itemize}
\vspace{0.2cm}
\item[6.] An integration time. How long is long enough?
\begin{itemize}
\item Old Stan: No U-Turn Criterion / No U-Turn Sampler (NUTS) \\
\vspace{0.2cm}
``stop when we start heading back toward where we started.''
\vspace{0.2cm}
\item New Stan: eXhaustive HMC (XHMC/XMC/better NUTS) \\
\vspace{0.2cm}
``stop when it looks like autocorrelation should be low.''
\vspace{0.2cm}
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Why Hamiltonian Monte Carlo?}

The long answer:
\begin{itemize}
\item \textbf{Everything You \emph{Should} Have Learned About MCMC} \\
(Michael Betancourt) \\
\url{https://www.youtube.com/watch?v=DJ0c7Bm5Djk&feature=youtu.be&t=4h40m10s}
\item \textbf{A Conceptual Introduction to HMC} (Michael Betancourt) \\
\url{https://arxiv.org/pdf/1701.02434.pdf}
\item \textbf{Hamiltonian Monte Carlo for Hierarchical Models}
(Michael Betancourt and Mark Girolami) \\
\url{https://arxiv.org/pdf/1312.0906.pdf}
\end{itemize}

\vspace{0.5cm}

The short answer:
\begin{itemize}
\item Works in high dimensions.
\item More robust.
\item Makes noise when it fails.
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Works in high dimensions}
We are interested in expecations of the form $\int f(\bm{q}){\color{red}\pi(\bm{q})}d\bm{q}$.
\begin{itemize}
\item Naively: focus on areas where ${\color{red}\pi(\bm{q})}$ (density) is large.
\item Better: where where ${\color{red}\pi(\bm{q})}d\bm{q}$ (mass) is large; ``typical set.''
\end{itemize}
\begin{center}
Typical Set:\\
\includegraphics[height=0.4\textheight]{typicalset.png}
\end{center}
In high dimensions, this is essentially a surface.
\begin{itemize}
\item Random walk methods have to take tiny steps.
\item Gibbs methods take a long time to move around the surface.
\item Modes are far away from mass $\to$ mode-based methods fail.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Robustness and Noisy Failure}
HMC has guaranteed geometric ergodicity in a larger class of target densities than alternatives.

\vspace{0.5cm}

When geometric ergodicity fails, HMC often won't sample due to numerically infinite gradients (``divergent transitions'').
\begin{center}
\includegraphics[height = 0.4\textheight]{divergent.png}
\end{center}
\begin{itemize}
\item Caused by weird posterior geometries (reparameterize).
\item Common in hierarchical models.
\item Also a problem in Gibbs samplers, but they still give output.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Using Stan: Resources}
How to install Stan and \verb0rstan0 (Follow the directions carefully!):
\begin{itemize}
\item \url{https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started}
\end{itemize}

Stan manual (it's very good and constantly being improved):
\begin{itemize}
\item \url{https://github.com/stan-dev/stan/releases/download/v2.14.0/stan-reference-2.14.0.pdf}
\end{itemize}

Links to the manual, examples, tutorials, and case studies:
\begin{itemize}
\item \url{http://mc-stan.org/documentation/}
\end{itemize}

\verb0rstan0 documentation:
\begin{itemize}
\item \url{http://mc-stan.org/interfaces/rstan.html}
\end{itemize}

A brief guide to Stan's warnings:
\begin{itemize}
\item \url{http://mc-stan.org/misc/warnings.html}
\end{itemize}

A shorter intro with a different emphasis:
\begin{itemize}
\item \url{http://mlss2014.hiit.fi/mlss_files/2-stan.pdf}
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Using Stan: A Simple Example}
Running example: modeling U.S. county level income as a function of covariates.\\~\\

(See \verb0stanintro.R0 for construction of covariates.)
\begin{align*}
y_i & = \mbox{mean household income in county $i$},\\
\bm{x}_i' & = \mbox{education and race covariates for county $i$}.
\end{align*}
The regression model:
\begin{align*}
y_i \stackrel{ind}{\sim} \mathrm{N}(\alpha + \bm{x}_i'\bm{\beta}, \sigma^2)
\end{align*}
for $i=1,2,\dots,N$.
\end{frame}


\begin{frame}[fragile]
\frametitle{Using Stan: A Simple Example}
Define the model in a \verb0.stan0 file, e.g.:
\begin{verbatim}
data {
  int<lower = 1> n_obs;
  int<lower = 1> n_cov;
  vector[n_obs] y;
  matrix[n_obs, n_cov] x;
}
parameters {
  real alpha;
  vector[n_cov] beta;
  real<lower = 0> sigma;
}
model {
  y ~ normal(alpha + x*beta, sigma);
  alpha ~ normal(60000, 20000);
  beta ~ normal(0, 20000);
  sigma ~ student_t(5, 0, 20000);
}
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{The .stan File}
Defines a target density as a function of data and parameters.
\begin{itemize}
\item Data: all things that are fixed during MCMC, including prior hyperparameters and what we normally think of as ``data''.
\item Parameters: all things we want/need to sample from.\\~\\
\end{itemize}
The file is composed of several ``program blocks'' in a specific order.
\vspace{-.5cm}
\begin{itemize}
\item ``parameters'' and ``model'' blocks are mandatory.
\item ``data'' block is necessary to read data into Stan.
\item And several others.
\end{itemize}
\begin{verbatim}
data {
  // define all variables to be read into Stan here
}
parameters {
  // define all parameters of the target density here
}
model {
  // define model as a function parameters and data here
}
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Defining variables}
A variable can be defined at the beginning of any block,\\
or at the beginning of any code chunk (\verb0{...}0).\\~\\
\begin{itemize}
\item Stan has two basic data types: \verb0int0 and \verb0real0.
\item Vectors and matrices are collections of \verb0real0s.
\item Can define arrays of \verb0int0s or \verb0real0s... or vectors or whatever.
\item Must define every variable that Stan will use.
\item Arrays, vectors, and matrices must have defined dimensions.
\item Can (and should) specify any relevant bounds for all variables (error checking \& more).
\item Stan uses \verb0R0 style 1-based indexing.\\~\\
\end{itemize}
Basic syntax: \verb0int n;0 or \verb0real y;0.
\end{frame}

\begin{frame}[fragile]
\frametitle{Some example variable definitions}
\begin{verbatim}
int<lower = 1> A;  // A >= 1 (constraints are inclusive)
real<upper = 0> B; // B <= 0
real<lower = 0, upper = 1> C; // 0 <= C <= 1
vector[10] D;                 // vector of 10 reals
vector<lower = 0>[10] E;      // vector of 10 reals >= 0
row_vector[5] F;              // row vector of 5 reals
matrix[10, 5] H;              // 10x5 matrix of reals
cov_matrix I[2];              // 2x2 PD matrix
corr_matrix J[2];             // 2x2 PD matrix
cholesky_factor_cov K[2];     // lower triangular
cholesky_factor_corr L[2];    // lower triangular
simplex M[5]; // each 0 < M[i] < 1; sum_i M[i] = 1
\end{verbatim}
Note: \verb0C0-style syntax: end statements with a semicolon (\verb0;0),\\
\ \ \ \ \ \ \ \, and `\verb0//0' comments the rest of the line (like \verb0#0 in \verb0R0).
\end{frame}

\begin{frame}[fragile]
\frametitle{Array / matrix indexing: mostly like R}
Indexing order: array dimensions, then row, then column.
\begin{verbatim}
real A[N];         // N-dim array
vector[N] B;       // N-dim vector
matrix[N, M] C;    // NxM matrix
vector[N] D[M];    // M N-dim vectors
matrix[N, M] E[K]; // K NxM matrices
A[3]; B[3];        // access 3rd element
A[1:3]; B[1:3];    // 1st - 3rd elements
A[ii]; B[ii]; // if ii = [1, 3], 1st and 3rd elements
C[1,2];            // 1st row / 2nd column
D[1,2];            // 2nd element of 1st vector
C[1];              // first row
D[1];              // first vector
C[,1];             // first column
D[,1];             // vector of 1st elements
E[1,1:4,1:4];      // top left 4x4 submatrix of 1st matrix
\end{verbatim}
...and combinations of the above.
\end{frame}

\begin{frame}[fragile]
\frametitle{Data block}
Define all data that will be read into Stan. Ex: \verb0regression.stan0
\begin{verbatim}
data {
  int<lower = 1> n_obs;
  int<lower = 1> n_cov;
  vector[n_obs] y;
  matrix[n_obs, n_cov] x;
  real beta_prior_mn;
  real<lower = 0> beta_prior_sd;
  real alpha_prior_mn;
  real<lower = 0> alpha_prior_sd;
  real<lower = 0> sig_prior_scale;
  real<lower = 0> sig_prior_df;
}
\end{verbatim}
This block \emph{only} consists of variable definitions.\\~\\

Any constraints are checked once before the sampler is run.
\end{frame}

\begin{frame}[fragile]
\frametitle{Parameters block}
Define all parameters in the model.\\~\\
Example from \verb0regression.stan0:
\begin{verbatim}
parameters {
  real alpha;
  vector[n_cov] beta;
  real<lower = 0> sigma;
}
\end{verbatim}
This block \emph{only} consists of variable definitions.\\~\\

Stan automatically transforms constrained parameters to unconstrained Euclidean space and computes the relevant Jacobian. \vspace{-.5cm}
\begin{itemize}
\item Stan can hand simple constraints stated in terms of lower \& upper bounds, e.g. \verb0real<lower = mu_x> mu_y;0,
\item ...and certain hardcoded complex constraints, e.g. covariance and correlation matrices, Cholesky factors of both, etc.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Model block}
Define the model in terms of parameters and data.\\~\\
Example from \verb0regression.stan0:
\begin{verbatim}
  y ~ normal(alpha + x*beta, sigma);
  alpha ~ normal(alpha_prior_mn, alpha_prior_sd);
  beta ~ normal(beta_prior_mn, beta_prior_sd);
  sigma ~ student_t(sig_prior_df, 0, sig_prior_scale);
\end{verbatim}
Each sampling statement (`\verb0~0') adds the relevant quantity to the target log-density. The LHS must be a previously defined variable (data, parameter, transformed data, or transformed parameter blocks).\\~\\

\textbf{The LHS cannot be an arbitrary function of a variable.}\\~\\

Use transformed data/parameters blocks to solve this (more later).
\end{frame}

\begin{frame}[fragile]
\frametitle{Arithmetic operators}
`\verb0*0' is matrix multiplication when its arguments are not scalars. Similarly `\verb0/0' and `\verb0\0' are matrix division:
\begin{align*}
A * B = AB, && A / B = AB^{-1}, && A \backslash B = A^{-1}B
\end{align*}
Use `\verb0 .* 0' and `\verb0 ./ 0' for elementwise operations.\\
\ \ \ \ ...but give these operators some space:\\
\ \ \ \ \verb0A.*B0 will throw an error, \verb0A .* B0 will not.\\~\\

Otherwise, most things work just like \verb0R0, except Stan is finicky about dimensions matching (this is good for catching errors).
\begin{itemize}
\item other differences: \verb0||0 for `or', \verb0&&0 for 'and', \verb0X'0 for $X$ transpose.\\~\\
\end{itemize}

Check the manual for efficient specialized functions for many common linear algebra (and other) operations, e.g.:
\begin{verbatim}
crossprod(); tcrossprod(); dot_product(); quad_form();
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Vectorization vs loops}
Sampling statements are vectorized when it makes sense.\\~\\

I.e. the following are equivalent ways of coding $y$'s model:
\begin{itemize}
\item \verb0y ~ normal(alpha + x*beta, sigma);0\
\item
\begin{verbatim}
for(i in 1:n_obs){ // same as in R
  y[i] ~ normal(alpha + x[i]*beta, sigma);
}
\end{verbatim}
\end{itemize}
Stan is written in \verb0C++0 so for loops are fast...\\~\\

But the \verb0autodiff0 library is much faster on vectorized models.
\begin{itemize}
\item Much faster gradient computations.
\item Sampling is cheaper per-iteration (per-leapfrog step).\\~\\
\end{itemize}

Upshot: vectorize wherever you can (see manual).
\end{frame}

\begin{frame}[fragile]
\frametitle{Fitting the model in R}
Fitting a regression:
\begin{verbatim}
## create list of all variables in the data block
regdat <- list(n_obs = nrow(codata), n_cov = ncol(x.base),
          y = codata$income.mean, x = x.base,
          beta_prior_mn = 0, beta_prior_sd = 20000,
          alpha_prior_mn = 60000, alpha_prior_sd = 20000,
          sig_prior_scale = 20000, sig_prior_df = 5)

## initialize: create the model and check data constraints
## (takes a good 15-30 seconds)
regfit0 = stan("regression.stan", data = regdat,
            chains = 1, iter = 1)
## ignore compiler warnings

## fit the model (only need to initialize once)
regfit = stan(fit = regfit0, data = regdat, cores = 4,
           chains = 4, warmup = 2000, iter = 4000)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{The stan() function in rstan}
Required arguments:
\begin{itemize}
\item A model --- ``.stan'' file, or \verb0fit = stanfit0, a \verb0stan0 object.
\item data --- list of all variables in the data block of the model.\\
\ \ \ \ \ \ \ \ \ \ \        (only required if the model has a data block)\\~\\
\end{itemize}
Useful named arguments and their defaults:
\begin{itemize}
\item \verb0cores = 10 --- number of cores to use, w/ \verb0parallel0 backend.
\vspace{-.3cm}
\begin{itemize}
\item Parallelizes \emph{across} chains, not within.
\end{itemize}
\item \verb0chains = 40 --- number of chains.
\item \verb1iter = 40001 --- total number of iterations per chain.
\item \verb0warmup = iter/20 --- iterations used for tuning / burn-in.\\~\\
\end{itemize}
Starting values, tuning, etc., taken care of automatically, but much of this is exposed in \verb0stan()0.
\end{frame}

\begin{frame}[fragile]
\frametitle{Running stan()}
\begin{verbatim}
> regfit0 = stan("regression.stan", data = regdat,
+                chains = 1, iter = 1)
...
warning: "BOOST_NO_CXX11_RVALUE_REFERENCES" redefined
...
\end{verbatim}
\begin{itemize}
\item Ignore compiler warnings like the one above.
\end{itemize}
\begin{verbatim}
> regfit = stan(fit = regfit0, data = regdat, cores = 4,
+          chains = 4, warmup = 2000, iter = 4000)
...
Chain 1, Iteration: 4000 / 4000 [100%]  (Sampling)
 Elapsed Time: 51.28 seconds (Warm-up)
               38.532 seconds (Sampling)
               89.812 seconds (Total)
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Traceplots}
\verb0> traceplot(regfit, pars = c("alpha", "beta"))0
\begin{center}
\includegraphics[width = 0.99\textwidth]{regtrace1.png}
\end{center}
\end{frame}

\begin{frame}[fragile]
\frametitle{More Traceplots}
\begin{verbatim}
> traceplot(regfit,
+  pars = c(paste("beta[", 1:3, "]", sep=""), "sigma"))
\end{verbatim}
\begin{center}
\includegraphics[width = 0.99\textwidth]{regtrace2.png}
\end{center}
\end{frame}

\begin{frame}[fragile]
\frametitle{Posterior Summaries}
\tiny{
\begin{verbatim}
> summary(regfit, pars = c("alpha", "beta", "sigma"))$summary
                  mean      se_mean          sd        2.5%          25%
alpha     71399.869821  84.94908998 3938.516308  63689.9678  68746.88233
beta[1]   -3551.058747   6.26461887  489.504511  -4522.1539  -3873.28857
beta[2]   63251.705576  64.16739808 5145.175179  53114.7035  59754.63962
beta[3]   22419.975765  45.58024712 3771.151469  14939.3834  19849.17895
beta[4]  -11424.187414  91.62543117 7604.783988 -26123.4236 -16528.42301
beta[5]       8.222437   1.30817829  111.458389   -212.2165    -67.37313
beta[6]  -26285.741524  96.51512551 5748.283846 -37469.5113 -30101.28221
beta[7]  -63841.417321 120.38712152 6874.102743 -77235.3078 -68536.56421
beta[8]  -12118.477929  44.03565139 2230.542000 -16481.2903 -13615.97501
beta[9]  -26595.696705  69.79832336 3910.493582 -34157.9793 -29214.67677
beta[10] -17089.068001 112.51467271 6941.976014 -30492.6935 -21873.41458
beta[11]  81749.431910 115.67307042 5624.916358  70767.5652  77925.54102
sigma      9049.495580   1.40047729  115.141941   8831.0488   8970.70867
lp__     -30209.057602   0.04393621    2.559298 -30214.9288 -30210.55878
                   50%          75%       97.5%    n_eff      Rhat
alpha     71391.759269  74043.63345  79140.4871 2149.551 1.0008773
beta[1]   -3550.100262  -3217.50867  -2593.4421 6105.540 0.9998866
beta[2]   63251.123712  66745.33228  73248.2005 6429.415 1.0003574
beta[3]   22429.869178  24978.31383  29683.4286 6845.333 0.9996002
beta[4]  -11532.083317  -6253.10872   3440.9387 6888.770 1.0001511
beta[5]       7.641381     83.44036    228.8966 7259.248 1.0005791
beta[6]  -26343.125976 -22457.52170 -15092.0916 3547.200 0.9997183
beta[7]  -63924.700591 -59193.35899 -50219.5088 3260.408 1.0005047
beta[8]  -12103.518865 -10620.56975  -7700.9977 2565.736 1.0007448
beta[9]  -26561.026938 -23967.36268 -18947.1339 3138.869 1.0006779
beta[10] -17026.910436 -12399.28093  -3668.6178 3806.693 1.0002487
beta[11]  81754.468636  85563.39644  92635.6943 2364.654 1.0005060
sigma      9047.818999   9127.68236   9276.6519 6759.506 0.9997143
lp__     -30208.734992 -30207.18270 -30205.0836 3393.100 1.0014618
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{More Posterior Summaries}
\verb0lp__0: value of the log posterior at every iteration.\\
\verb0n_eff0: effective sample size.\\
\begin{itemize}
\item equivalent number of iid draw for that parameter.
\end{itemize}
\verb0Rhat0: potential scale reduction factor.
\begin{itemize}
\item convergence diagnostic based on multiple chains.
\item \verb0Rhat0 $< 1.01$: no evidence against convergence \emph{for that parameter}.\\~\\
\end{itemize}
\verb0shinystan0 \verb0R0 package:
\begin{itemize}
\item \url{http://mc-stan.org/interfaces/shinystan}\\~\\
\end{itemize}
Extract posterior draws into a named list:
\vspace{-0.2cm}
\tiny{
\begin{verbatim}
> regfitdraws <- extract(regfit)
> str(regfitdraws, 1)
List of 4
 $ alpha: num [1:8000(1d)] 65076 74078 62241 78395 68850 ...
 $ beta : num [1:8000, 1:11] -2995 -3533 -3492 -3856 -3685 ...
 $ sigma: num [1:8000(1d)] 9174 8926 9124 9074 8982 ...
 $ lp__ : num [1:8000(1d)] -30212 -30208 -30211 -30206 -30210 ...
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{Transformed data and parameters}
Transformed data: transform once before running MCMC.\\
Transformed parameters: transform once per leapfrog step.\\
\begin{verbatim}
data { ... }
transformed data {
  vector[n_obs] log_y;
  log_y = log(y);
}
parameters = { ... }
transformed parameters {
  real<lower = 0> sigma;
  sigma = sqrt(sigma2);
}
model { // implicit U(-Inf, Inf) prior on mu
  log_y ~ normal(mu, sigma);
  sigma2 ~ inv_gamma(0.5, 0.5); // this is a bad prior
}
\end{verbatim}
\end{frame}

\begin{frame}[fragile]
\frametitle{Other places to put transformations}
Model block:
\begin{itemize}
\item Computed once per leapfrog step --- useful for intermediate quantities you don't want MCMC draws for.
\item No constraints allowed here.
\end{itemize}
{\tiny
\begin{verbatim}
model {
  real sigma;
  sigma = sqrt(sigma2);
  log_y ~ normal(mu, sigma);
  sigma2 ~ inv_gamma(0.5, 0.5); // this is a bad prior
}
\end{verbatim}
}
Generated quantities block:
\begin{itemize}
\item Computed once per MCMC iter --- useful for quantities you want draws for, but aren't needed for computing the posterior.
\item Can also put random draws here, e.g. for predictive dists.
\item Constraints allowed, but not necessary.
\end{itemize}
{\tiny
\begin{verbatim}
model { ... }
generated quantities {
  real muoversigma;
  muoversigma = mu / sqrt(sigma2);
}
\end{verbatim}
}
\end{frame}

\begin{frame}[fragile]
\frametitle{Where should I put my transformation for max efficiency?}
Transformed data block:
\begin{itemize}
\item All pure functions of data, or other intermediate quantities that don't depend on parameters.
\end{itemize}
Transformed parameters block:
\begin{itemize}
\item Only place to put transformations to automatically take into account the Jacobian.
\item Any quantity that is a function of parameters and on the LHS of a sampling statement (\verb0~0).
\item Avoid putting other quantities here because computing Jacobians is slow and unnecessary.
\end{itemize}
Model block:
\begin{itemize}
\item Intermediate quantities used on RHS of sampling statements.
\end{itemize}
Generated quantities block:
\begin{itemize}
\item Quantities you want MCMC draws for that aren't already in the parameters or transformed parameters block.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{All Possible Program Blocks}
Only the parameters and model blocks are mandatory,\\
but the blocks must appear in this order.
{\tiny
\begin{verbatim}
functions {
  // define user defined functions here (see manual)
}
data {
  // define all input (data / hyperparameter) variables here
}
transformed data {
  // create transformations of data and other intermediate
  // quantities that don't depend on parameters here
}
parameters {
  // define all model parameters here
}
transformed parameters {
  // create any needed transformations of parameters here
}
model {
  // create the log posterior density here
}
generated quantities {
  // create any other variables you want draws for here
}
\end{verbatim}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Program Block Characteristics}
\begin{center}
\includegraphics[width = 0.99\textwidth]{blockchars.png}
\end{center}
Stolen from \url{http://mlss2014.hiit.fi/mlss_files/2-stan.pdf}
\end{frame}

\begin{frame}[fragile]
\frametitle{Tricks for Better/Faster Samplers in Stan}
HMC/Stan works best when:
\begin{itemize}
\item All parameters are on similar scales.
\item Posterior geometries aren't ``weird''.
\end{itemize}
How to deal with these issues:
\begin{itemize}
\item Center and scale responses and covariates.
\item Work on the log scale, e.g. \verb0logy ~ normal(.,.);0 is better than \verb0y ~ lognormal(.,.);0
\item Reparameterize the model --- noncentered parameterizations.\\~\\
\end{itemize}

Centering and scaling data often drastically speeds up model fits because it makes adaptation easier.\\~\\

Reparameterization is often necessary to get a working sampler at all, especially in hierarchical models.
\end{frame}

\begin{frame}[fragile]
\frametitle{Regression with Centered and Scaled Data: Stan}
From \verb0regression_cs.stan0. Reduce fit time from 70 to 10 secs.\\~\\
Note: care taken to get the correct priors for the \verb0_cs0 parameters, and the correct parameter estimates back on original scale.
{\tiny
\begin{columns}
\begin{column}{0.45\textwidth}
\begin{verbatim}
transformed data {
  ...
  // center and scale y
  y_mn = mean(y);
  y_sd = sd(y);
  y_cs = (y - y_mn)/y_sd;

  // center and scale x
  for(i in 1:n_cov){
    x_mn[i] = mean(x[,i]);
    x_sd[i] = sd(x[,i]);
    x_cs[,i] = (x[,i] - x_mn[i]) / x_sd[i];
  }

  // priors on _cs parameters
  x_mnsd = x_mn ./ x_sd;
  beta_cs_prior_mn = x_sd * beta_prior_mn / y_sd;
  beta_cs_prior_sd = x_sd * beta_prior_sd / y_sd;
  alpha_cs_prior_mn = (alpha_prior_mn-y_mn)/y_sd;
  alpha_cs_prior_sd = alpha_prior_sd / y_sd;
  sig_cs_prior_scale = sig_prior_scale / y_sd;
}
\end{verbatim}
\end{column}
\begin{column}{0.55\textwidth}
\begin{verbatim}
parameters {
  real alpha_cs;
  vector[n_cov] beta_cs;
  real<lower = 0> sigma_cs;
}
model {
  y_cs ~ normal(alpha_cs + x_cs*beta_cs, sigma_cs);
  beta_cs ~ normal(beta_cs_prior_mn, beta_cs_prior_sd);
  alpha_cs ~ normal(alpha_cs_prior_mn +
        dot_product(x_mnsd, beta_cs), alpha_cs_prior_sd);
  sigma_cs ~ student_t(sig_prior_df, 0, sig_cs_prior_scale);
}
generated quantities {
  real alpha;
  vector[n_cov] beta;
  real<lower = 0> sigma;

  beta = (beta_cs ./ x_sd) * y_sd;
  alpha = alpha_cs * y_sd -
          dot_product(x_mn, beta) + y_mn;
  sigma = sigma_cs * y_sd;
}
\end{verbatim}
\end{column}
\end{columns}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Dealing with Hierarchical Models}
Example: same regression, now with region-level random intercepts.\\~\\
Let $r[i]$ denote the state that county $i$ is in, and $\alpha_r$ denote the intercept for state $r$.\\~\\
Now we specify the model directly on centered and scaled $y_i$ and $\bm{x}_i'$:
\begin{align*}
y_i | \bm{\alpha} \stackrel{ind}{\sim} \mathrm{N}(\alpha_{r[i]} + \bm{x}_i'\bm{\beta}, \sigma^2), && \alpha_r \stackrel{iid}{\sim} \mathrm{N}(\mu_{\alpha}, \sigma_{\alpha}^2),
\end{align*}
for $i=1,2,\dots,N$, $r=1,2,\dots,R$.
\end{frame}

\begin{frame}[fragile]
\frametitle{Random Intercept Regression in Stan}
From \verb0rand_intercept_reg.stan0 (about 20 seconds to fit):
{\tiny
\begin{verbatim}
parameters {
  vector[n_cov] beta;
  real<lower = 0> sigma;
  vector[n_region] alpha;
  real mu_alpha;
  real<lower = 0> sigma_alpha;
}
model {
  y_cs ~ normal(region*alpha + x_cs*beta, sigma);
  alpha ~ normal(mu_alpha, sigma_alpha);
  beta ~ normal(0, 10);
  sigma ~ student_t(5, 0, 10);
  mu_alpha ~ normal(0, 10);
  sigma_alpha ~ student_t(5, 0, 10);
}
\end{verbatim}}
Problems:
{\tiny
\begin{verbatim}
> randintfit <- stan(fit = randintfit0, data = randintdat, cores = 4, chains = 4,
+                    warmup = 2000, iter = 4000, open_progress = FALSE)
...
[1] "The following numerical problems occured the indicated number of times on chain 4"
                                                                                count
Exception thrown at line 37: normal_log: Scale parameter is 0, but must be > 0!     3
[1] "When a numerical problem occurs, the Hamiltonian proposal gets rejected."
[1] "See http://mc-stan.org/misc/warnings.html#exception-hamiltonian-proposal-rejected"
[1] "If the number in the 'count' column is small,  do not ask about this message on stan-users."
Warning messages:
1: There were 1146 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See
http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup
2: Examine the pairs() plot to diagnose sampling problems
\end{verbatim}}
\end{frame}

\begin{frame}[fragile]
\frametitle{Guide to Stan's Warnings}
Summarizing from \url{http://mc-stan.org/misc/warnings.html}\\~\\

Exception ... Hamiltonian proposal rejected: {\tiny\verb9Exception thrown at line 37: normal_log: Scale parameter is 0, but must be > 0!9}.
\begin{itemize}
\item Not a problem if the proportion of iters with exceptions is low.
\item If the proportion is high: indication of a problem w/ the model.\\
\begin{itemize}
\item[] e.g. maybe a hierarchical variance should be zero.\\~\\
\end{itemize}
\end{itemize}

Low Bayesian Fraction of Missing Information:
\begin{itemize}
\item Indicates that the adaptation turned out poorly and the chain did not properly explore the posterior.
\item Can solve by increasing \verb0warmup0 or reparameterizing.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Focus on Divergent Transitions}
Divergent transitions after warmup: {\tiny\verb91: There were 1146 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help.9}
\vspace{-0.4cm}
\begin{itemize}
\item Indicates that geometric ergodicity is breaking down. \textbf{\emph{Just one divergent transition is enough to be concerning.}}
\item Solve via reparameterization or increasing \verb0adapt_delta0.\\
\verb0stan(..., control = list(adapt_delta = .99))0\\~\\
\end{itemize}

\verb0adapt_delta0 is the target Metropolis acceptance rate, which is controlled via the leapfrog step size.\\~\\

Lowering the leapfrog step size forces the numerical integrator to follow the local curvature of the target distribution more closely...\\~\\

But this doesn't always work! Reparameterization can solve it.
\end{frame}

\begin{frame}[fragile]
\frametitle{The Problem with Hierarchical Models}
Good discussion in \textbf{HMC for Hierarchical Models}: \url{https://arxiv.org/pdf/1312.0906.pdf}\\~\\

In a nutshell: these posterior distributions have weird geometries.
\begin{itemize}
\item Parameters are highly correlated, and global and local correlations may be very different.
\item Gibbs samplers have high autocorrelation.
\item Random Walk Metropolis samplers have small step sizes.
\item HMC more likely to have divergent transitions.
\item Geometric ergodicity hard \textbf{\emph{using any method}}.\\~\\
\end{itemize}
How to fix this?
\begin{itemize}
\item Gibbs: reparameterize, parameter expansion, interweaving.
\item HMC: reparameterize, decrease step size, Riemannian HMC.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Centered vs. noncentered parameterizations}
\begin{align*}
y_i &\sim \mathrm{N}(\alpha_{r[i]} + \bm{x}_i'\bm{\beta}, \sigma^2), && \alpha_r \sim \mathrm{N}(\mu_{\alpha}, \sigma_{\alpha}^2); && \mbox{(centered)}\\
y_i &\sim \mathrm{N}(\mu_{\alpha} + \sigma_{\alpha}\varepsilon_{r[i]} + \bm{x}_i'\bm{\beta}, \sigma^2), && \varepsilon_r \sim \mathrm{N}(0, 1). && \mbox{(noncentered)}\\
\end{align*}
Define the signal-to-noise ratio: $\sigma_{\alpha}^2/\sigma^2$.\\~\\

Centered parameterization:
\begin{itemize}
\item Usually the natural way to write the model.
\item Results in easy MCMC when the signal-to-noise ratio is large.\\~\\
\end{itemize}
Noncentered parameterization:
\begin{itemize}
\item Usually found by centering and scaling: $\varepsilon_r = (\alpha_r - \mu_{\alpha})/\sigma_{\alpha}$.
\item Results in easy MCMC when the signal-to-noise ratio is small.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Noncentered Random Intercept Regression}
From \verb0rand_intercept_reg_noncen.stan0 (about 15 min to fit):
{\tiny
\begin{verbatim}
parameters {
  vector[n_cov] beta;
  real<lower = 0> sigma;
  vector[n_region] alpha_raw; // "noncentered" intercepts
  real mu_alpha;
  real<lower = 0> sigma_alpha;
}
model {
  vector[n_region] alpha;
  alpha = mu_alpha + sigma_alpha * alpha_raw;
  y_cs ~ normal(region*alpha + x_cs*beta, sigma);
  alpha_raw ~ normal(0, 1);
  beta ~ normal(0, 10);
  sigma ~ student_t(5, 0, 10);
  mu_alpha ~ normal(0, 10);
  sigma_alpha ~ student_t(5, 0, 10);
}
\end{verbatim}}
Less divergent transitions with this parameterization:
{\tiny
\begin{verbatim}
1: There were 701 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help.
\end{verbatim}}
Now try increasing \verb0adapt_delta0 (default = 0.8):
{\tiny
\begin{verbatim}
> randintncfit2 <- stan(fit = randintncfit, data = randintdat, cores = 4, chains = 4,
+                       warmup = 2000, iter = 4000, open_progress = FALSE,
+                       control = list(adapt_delta = 0.999))
\end{verbatim}}
yields 0 divergent transitions in about 15 minutes.
\end{frame}

\begin{frame}[fragile]
\frametitle{Stan vs Gibbs for Hierarchical Models}
Stan's major advantages:
\begin{itemize}
\item Stan tends to let you know when geometric ergodicity fails.
\item Stan tends to be more efficient.
\end{itemize}
\begin{center}
\includegraphics[width = 0.5\textwidth]{center.png}\\
\includegraphics[width = 0.9\textwidth]{hmctable.png}
\end{center}
From \textbf{HMC for Hierarchical Models}: \\
\ \ \ \ \ \url{https://arxiv.org/pdf/1312.0906.pdf}
\end{frame}

\begin{frame}[fragile]
\frametitle{Coming Soon: Riemannian HMC (and more)}
Good Riemannian HMC deals with all of this without user input.\\~\\

Momentum distribution $\bm{p}|\bm{q} \sim \mathrm{N}(\bm{0}, \bm{M}(\bm{q}))$.\\~\\
From the manual:
\begin{center}
\includegraphics[width=0.5\textwidth]{stanfuture.png}
\end{center}
Good results from setting $\bm{M}$ to a function of the Hessian of $\pi(\bm{q})$.\\
 $\implies$ need second order \verb0autodiff0. They're working on it.
\end{frame}

\begin{frame}
      \begin{center}

        \font\endfont = cmss10 at 20.40mm
        \color{Red}
        \endfont
        \baselineskip 20.0mm

        Thank you!\\~\\

        Questions?

      \end{center}
\vspace{.4cm}
Matthew Simpson \hfill themattsimpson@gmail.com
\end{frame}
\end{document}

