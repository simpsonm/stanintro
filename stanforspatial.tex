\documentclass[xcolor=dvipsnames]{beamer}
\makeatletter\def\Hy@xspace@end{}\makeatother 
\usepackage{graphicx, color, amssymb, amsmath, bm, rotating, graphics,
epsfig, multicol, amsthm}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage[ansinew]{inputenc}
\usepackage[authoryear]{natbib}
%\newcommand{\newblock}{}  %needed to make beamer and natbib play nice
\usepackage{tikz}
%\usetheme{Boadilla}
%\usecolortheme{lily}
 \usecolortheme[named=Red]{structure}
% \setbeamercolor{structure}{bg=black}
% \setbeamercolor{structure}{fg=Goldenrod}
% \setbeamercolor{title}{bg=Black}
% \setbeamercolor{frametitle}{bg=Black, fg=Goldenrod}
% \setbeamercolor{title in head/foot}{fg=Black, bg=Goldenrod}
% \setbeamercolor{author in head/foot}{fg=Goldenrod, bg=Black}
% \setbeamercolor{institute in head/foot}{fg=Goldenrod, bg=Black}
% \setbeamercolor{date in head/foot}{fg=Goldenrod, bg=Black}
\setbeamercovered{transparent=0}
\beamertemplatenavigationsymbolsempty

\title[Stan for Spatial]{Using Stan for Spatial and Spatio-Temporal Modeling}

%\subtitle{}
\author[Matt Simpson]{Matthew Simpson}
\institute[Mizzou Statistics]{Department of Statistics, University of Missouri}

\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}
\frametitle{Stan is...}

\begin{columns}[c]
 \begin{column}[c]{0.5\textwidth}

\begin{itemize}
\item[] 
\begin{center}
\includegraphics[width=0.5\textwidth]{stan.jpg}
\end{center}
\end{itemize}
\end{column}

\begin{column}[c]{0.5\textwidth}
\begin{itemize}
\item[] \textbf{Stanislaw Ulam}, inventor of Monte Carlo methods.\\
\end{itemize}
\end{column}
\end{columns}

\vspace{0.2cm}

\begin{itemize}
\item[] \url{http://mc-stan.org/}

\vspace{0.2cm}

\item[] A \textbf{probabilistic programming language} that implements {\color{red}\textbf{Hamiltonian Monte Carlo (HMC)}}, variational Bayes, and penalized maximum likelihood estimation.\\

\vspace{0.2cm}

\item[] Available on Linux, Mac, and Windows with interfaces in \textbf{R}, \textbf{Python}, shell (command line), MATLAB, Julia, Stata, and Mathematica.

\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Markov chain Monte Carlo (MCMC)}
Goal: sample from some density $\pi(\bm{q})$.\\

\vspace{0.5cm}

Create a Markov chain with transition density $k(\bm{q}'|\bm{q})$.
\begin{itemize}
\item Start with arbitrary $\bm{q}^{(0)}$ and repeatedly sample $\bm{q}^{(t+1)} \sim k(\bm{q}'|\bm{q}^{(t)})$.\\

\vspace{0.2cm}

\item Under some conditions $\bm{q}^{(t)} \to \bm{q}$ in distribution.

\vspace{0.2cm}

\item Additionally with {\color{red}geometric ergodicity}:
\begin{align*}
  \frac{1}{T}\sum_{t=1}^T f(\bm{q}^{(t)}) \to \mathrm{N}\left(\mathrm{E}[f(\bm{q})], \frac{\mathrm{var}[f(\bm{q})]}{ESS}\right).
\end{align*}
\end{itemize}

\vspace{0.5cm}

Auxillary variable MCMC: construct a variable $\bm{p}$ with joint density $\pi(\bm{q},\bm{p}) = \pi(\bm{p}|\bm{q})\pi(\bm{q})$.
\begin{itemize}
\item Construct a Markov chain for $(\bm{q}, \bm{p})$ and throw away the sampled $\bm{p}^{(t)}$s.
\vspace{0.2cm}
\item Ex: data augmentation, slice sampling, HMC, etc.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{HMC in Theory}
Construct $\bm{p}$ in a special way. Let $\bm{q},\bm{p}\in \Re^n$ and:
\begin{itemize}
\item[] $V(\bm{q})\phantom{,\bm{p}} = -\log \pi(\bm{q})$ --- \emph{potential energy}.
\item[] $T(\bm{q}, \bm{p}) = - \log \pi(\bm{p} | \bm{q})$ --- \emph{kinetic energy}.
\item[] $H(\bm{q}, \bm{p}) = V(\bm{q}) + T(\bm{q}, \bm{p})$ --- \emph{Hamiltonian}, total energy. 
\end{itemize}
where $\bm{p}$ denotes \emph{position} and $\bm{q}$ denotes \emph{momentum}.

\vspace{0.3cm}

Energy-preserving evolution in time is defined by Hamilton's equations:
\begin{align*}
\frac{\mathrm{d}\bm{p}}{\mathrm{d} t} = - \frac{\partial H}{\partial \bm{q}}; && \frac{\mathrm{d}\bm{q}}{\mathrm{d} t} = + \frac{\partial H}{\partial \bm{p}}.
\end{align*}
How to implement HMC (in theory):
\begin{enumerate}
\item Sample $\bm{p}' \sim \pi(\bm{p}|\bm{q}^{(t)})$.
\item Run Hamiltonian evolution forward in time from $(\bm{q}^{(t)}, \bm{p}')$ for a some amount of \emph{integration time} to obtain $(\bm{q}^{(t+1)}, \bm{p}^{(t+1)})$.
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{HMC in Pictures}
\begin{center}
\includegraphics[height=0.5\textheight]{hmc.png}
\end{center}

HMC samples a level set, then deterministically moves along that set.

\vspace{0.3cm}

Long integration time $\implies$ essentially zero autocorrelation in the chain.

\vspace{0.3cm}

(Picture stolen from \url{https://arxiv.org/pdf/1601.00225.pdf})
\end{frame}

\begin{frame}{fragile}
\frametitle{HMC in Practice}
To implement HMC for a differentiable target $\pi(\bm{q})$ you need:
\vspace{0.2cm}
\begin{enumerate}
\item No discrete valued parameters in $\bm{q}$.  
\begin{itemize}
\item Usually can integrate them out, e.g. mixture models.
\end{itemize}
\vspace{0.2cm}
\item No constrained parameters in $\bm{q}$. 
\begin{itemize}
\item Stan: transform and compute the log-Jacobian automatically.
\end{itemize}
\vspace{0.2cm}
\item The gradient vector of $\log \pi(\bm{q})$.
\begin{itemize}
\item Stan: use C++ autodiff library to do this automatically and accurately.
\end{itemize}
\vspace{0.2cm}
\item Choose a kinetic energy, i.e. $\pi(\bm{p}|\bm{q})$.
\begin{itemize}
\item Stan: $\mathrm{N}(\bm{0}, \bm{M})$ and tune $\bm{M}$ during warmup. (Typical HMC)
\item More intelligent: $\bm{M}(\bm{q})$. (Riemannian HMC; future Stan)
\end{itemize}
\end{enumerate}
\begin{align*}
{\color{red}\Huge{\vdots}}
\end{align*}
\end{frame}

\begin{frame}
\frametitle{HMC in Practice (continued)}
To implement HMC for a differentiable target $\pi(\bm{q})$ you need:
\vspace{0.2cm}
\begin{enumerate}
\item[5.] Numerical integrator for Hamiltonian's equations.
\begin{itemize}
\item Need to make an adjustment to the Hamiltonian flow and use a Metropolis correction to ensure detailed balance.
\vspace{0.2cm}
\item Typically use leapfrog integration $\implies$ how many leapfrog steps ?
\vspace{0.2cm}
\item Stan: adapt number of steps to hit a target Metropolis acceptance rate.
\end{itemize}
\vspace{0.2cm}
\item[6.] An integration time. How long is long enough?
\begin{itemize}
\item Old Stan: No U-Turn Criterion / No U-Turn Sampler (NUTS) \\
\vspace{0.2cm}
``stop when we start heading back toward where we started.''
\vspace{0.2cm}
\item New Stan: eXhaustive HMC (XHMC/XMC/better NUTS) \\
\vspace{0.2cm}
``stop when it looks like autocorrelation should be low.''
\vspace{0.2cm}
\end{itemize}
\end{enumerate}
\end{frame}

\begin{frame}
\frametitle{Why Hamiltonian Monte Carlo?}

The long answer: 
\begin{itemize}
\item \url{https://www.youtube.com/watch?v=DJ0c7Bm5Djk&feature=youtu.be&t=4h40m10s}
\item \url{https://arxiv.org/pdf/1701.02434.pdf}
\item \url{https://arxiv.org/pdf/1312.0906.pdf}
\end{itemize}

\vspace{0.5cm}

The short answer: 
\begin{itemize}
\item Works in high dimensions. 
\item More robust. 
\item Makes noise when it fails. 
\end{itemize}

\end{frame}

\begin{frame}
\frametitle{Works in high dimensions}
We are interested in expecations of the form $\int f(\bm{q})\pi(\bm{q})d\bm{q}$.
\begin{itemize}
\item Naively: focus on areas where $f(\bm{q})\pi(\bm{q})$ is large.
\item Better: focus on areas where where $f(\bm{q})\pi(\bm{q})d\bm{q}$ is large.
\end{itemize}
\begin{center}
\includegraphics[height=0.4\textheight]{typicalset.png}
\end{center}
In high dimensions, this is essentially a surface.
\begin{itemize}
\item Random walk type methods (including Gibbs) often fail.
\item Methods based on modes often fail.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Robustness and Noisy Failure}
HMC has guaranteed geometric ergodicity in a larger class of target densities than alternatives.

\vspace{0.5cm}

When geometric ergodicity fails, HMC often won't sample due to numerically infinite gradients.
\begin{center}
\includegraphics[height = 0.4\textheight]{divergent.png}
\end{center}
\begin{itemize}
\item Caused by weird posterior geometries (reparameterize).
\item Common in hierarchical models.
\item Also a problem in Gibbs samplers, but they still give output.
\end{itemize}
\end{frame}

\begin{frame}[fragile]
\frametitle{Using Stan}
Define the model in a \verb0.stan0 file:
\begin{verbatim}
data {
  int nobs;
  vector[nobs] y;
}
parameters {
  real mu;
  real<lower = 0> sigma;
}
model {
  y ~ normal(mu, sigma);
  mu ~ normal(0, 10.0);
  sigma ~ normal(0.0, 10.0);
}
\end{verbatim}
\end{frame}

\end{document}
